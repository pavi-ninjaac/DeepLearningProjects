{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading https://files.pythonhosted.org/packages/70/a8/e52a82936be6d5696fb06c78450707c26dc13df91bb6bf49583bb9abbaa0/opencv_python-4.5.1.48-cp37-cp37m-win_amd64.whl (34.9MB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in d:\\anaconda\\installed_files\\lib\\site-packages (from opencv-python) (1.16.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.1.48\n"
     ]
    }
   ],
   "source": [
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "#tensorflow packages\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face Emotion Recognition\n",
    "#Here i am using my trained model, that is trained and saved as a h5 file\n",
    "faceDetection_model = 'D:\\pavi\\DeepLearningProjects\\Face_Emosion_Recognition\\pretrained_model\\Face_Detection_TrainedModel\\haarcascade_frontalface_default.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 48, 48, 64)        1664      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 48, 48, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              9438208   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 11,707,591\n",
      "Trainable params: 11,706,695\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Emotion_Detction_model = 'D:\\pavi\\DeepLearningProjects\\Face_Emosion_Recognition\\pretrained_model\\Face_Emotion_model\\FER_vggnet.h5'\n",
    "vggnet = load_model(Emotion_Detction_model)\n",
    "vggnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Happy']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Angry']\n",
      "['Fear']\n",
      "['Fear']\n",
      "['Fear']\n",
      "['Happy']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Sad']\n",
      "['Neutral']\n",
      "['Angry']\n",
      "['Angry']\n",
      "['Neutral']\n",
      "['Angry']\n",
      "['Neutral']\n",
      "['Angry']\n",
      "['Neutral']\n",
      "['Sad']\n",
      "['Neutral']\n",
      "['Sad']\n",
      "['Angry']\n",
      "['Surprise']\n",
      "['Sad']\n",
      "['Angry']\n",
      "['Angry']\n",
      "['Angry']\n",
      "['Angry']\n",
      "['Angry']\n",
      "['Angry']\n",
      "['Sad']\n",
      "['Sad']\n"
     ]
    }
   ],
   "source": [
    "#defining the emotion classes for classification\n",
    "classes = np.array((\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"))\n",
    "\n",
    "#video capturing and classifing\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(faceDetection_model)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret,frame = video_capture.read()\n",
    "    \n",
    "    cv2.imshow('Original Video' , frame)\n",
    "    \n",
    "    gray = cv2.cvtColor(frame , cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    face = faceCascade.detectMultiScale(gray ,scaleFactor=1.1 , minNeighbors=5,)\n",
    "    \n",
    "    #draw rectangle around the face and cut the face only\n",
    "    for (x,y,w,h) in face:\n",
    "        \n",
    "        cv2.rectangle( gray , (x,y) , (x+w , y+h) , (0,255,255) , 2)\n",
    "        face_img = gray[ y:(y+h) , x:(x+w)]\n",
    "        x = cv2.resize(face_img, (48,48) , interpolation = cv2.INTER_AREA)\n",
    "        if np.sum([x])!=0:\n",
    "        #preprocessing\n",
    "            x = x.astype('float')/255.0 \n",
    "            x = image.img_to_array(x)\n",
    "            x = np.expand_dims(x , axis = 0)\n",
    "            \n",
    "        \n",
    "        #face_img = face_img.reshape(48,48)\n",
    "        \n",
    "        # prediction\n",
    "            p = vggnet.predict(x)\n",
    "            a = np.argmax(p,axis=1)\n",
    "            print(classes[a])\n",
    "        #cv2.imshow('croped image' , face_img)\n",
    "    #display the resulting frame \n",
    "    \n",
    "    cv2.imshow('Face Detected Video' , frame)\n",
    "    \n",
    "    #break the capturing\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
