{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rock_paper_seacer_model_training",
      "provenance": [],
      "mount_file_id": "1FPhUyKNdIY6zcuBhet_CjZTLfzbXAeZ9",
      "authorship_tag": "ABX9TyPQErml3eaKfE3m4ZJNLjDO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavi-ninjaac/DeepLearningProjects/blob/main/rock_paper_seacer_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCln9ayCYCjc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#keras \n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#keras layers\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Conv2D,MaxPooling2D,BatchNormalization,AveragePooling2D\n",
        "from keras.layers import Flatten,Dropout,Dense"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYRskvFYYZnD",
        "outputId": "e5555ace-fcdb-4ee5-cb34-f0ea7f9080a7"
      },
      "source": [
        "#read the dataset\n",
        "def generate_dataset(folder_name):\n",
        "  \"\"\"\n",
        "  :param folder_name: list of folder names to be considered for converting.\n",
        "\n",
        "  :return numpy array of data and target.\n",
        "  \"\"\"\n",
        "  data, target = [],[]\n",
        "  class_count = 0\n",
        "  # generate the img array.\n",
        "  for name in folder_name:\n",
        "    folder_path = f'/content/drive/MyDrive/Colab Notebooks/dataFiles/rock-paper-seacer/{name}'\n",
        "    print(f\"Collecting images from the {name}..........\")\n",
        "    \n",
        "    for image_name in os.listdir(folder_path):\n",
        "      \n",
        "      img = cv2.imread(f\"{folder_path}/{image_name}\")\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "      img = cv2.resize(img, (100, 100))\n",
        "      data.append(img)\n",
        "      target.append(class_count)\n",
        "\n",
        "\n",
        "    class_count += 1\n",
        "    print(f\"Done :) , Collecting images from the {name}..........\")\n",
        "  \n",
        "  # convert the array into numby array.\n",
        "  print(\"Converting to numpy array........\")\n",
        "  data = np.array(data)\n",
        "  target = np.array(target)\n",
        "\n",
        "  return data, target\n",
        "    \n",
        "  \n",
        "folder_name = ['rock', 'paper', 'scissors']\n",
        "data_array, target_array = generate_dataset(folder_name)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting images from the rock..........\n",
            "Done :) , Collecting images from the rock..........\n",
            "Collecting images from the paper..........\n",
            "Done :) , Collecting images from the paper..........\n",
            "Collecting images from the scissors..........\n",
            "Done :) , Collecting images from the scissors..........\n",
            "Converting to numpy array........\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H85E6H5-g2ps",
        "outputId": "23821ac3-b78f-4c4b-86d2-778ed925bc25"
      },
      "source": [
        "data_array.shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1499, 100, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhyzW0CNhVG9",
        "outputId": "7f5643eb-b6aa-40c1-ac1c-abfc08a56498"
      },
      "source": [
        "a= np.array([[1,2,3,],[1,3,4]])\n",
        "b = np.array(['a','b'])\n",
        "c=[]\n",
        "for i,ii in zip(a,b):\n",
        "  c.append([i,ii])\n",
        "c= np.array(c)\n",
        "print(c.shape)\n",
        "i = c[:,0]\n",
        "i"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([array([1, 2, 3]), array([1, 3, 4])], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNNmUDP0hUvO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E73_S2pAaa6z"
      },
      "source": [
        "def preprocess_dataset(data, target):\n",
        "  X_train,y_train,X_test,y_test,X_val,y_val = [],[],[],[],[],[] \n",
        "\n",
        "  print(\"Shuffling the dataset...............\")\n",
        "  dataset = []\n",
        "  for d, t  in zip(data,target):\n",
        "    dataset.append([d,t])\n",
        "  dataset = np.array(dataset)\n",
        "  np.random.shuffle(dataset)  \n",
        "\n",
        "  print(\"spliting all data...................\")\n",
        "  train_count, test_count, val_count = int(dataset.shape[0] * 8/10), int(dataset.shape[0] * 1/10), int(dataset.shape[0] * 1/10)\n",
        "  print(train_count, test_count, val_count)\n",
        "\n",
        "  # X_train, y_train = dataset[0:train_count+1, 0], dataset[0:train_count+1, 1]\n",
        "  # X_test, y_test = dataset[train_count+1:train_count+test_count+1 , 0], dataset[train_count+1:train_count+test_count+1 , 1]\n",
        "  # X_val, y_val = dataset[train_count+test_count+1:-1 , 0], dataset[train_count+test_count+1:-1 , 1]\n",
        "  for i in range(dataset.shape[0]):\n",
        "    d = dataset[i]\n",
        "    value, target = d[0], d[1]\n",
        "    \n",
        "    if i <= train_count:\n",
        "      X_train.append(value)\n",
        "      y_train.append(target)\n",
        "    elif i > train_count and i <= (train_count+test_count):\n",
        "      X_test.append(value)\n",
        "      y_test.append(target)\n",
        "    else:\n",
        "      X_val.append(value)\n",
        "      y_val.append(target)\n",
        "      \n",
        "  print(\"Converting to numpy array>>>>>>>>>>>>>>>>>>\")\n",
        "  #convert list to numpy array\n",
        "  X_train = np.array(X_train,'float32')  \n",
        "  y_train = np.array(y_train,'float32')  \n",
        "  X_test = np.array(X_test,'float32')  \n",
        "  y_test = np.array(y_test,'float32')\n",
        "  X_val = np.array(X_val,'float32')  \n",
        "  y_val = np.array(y_val,'float32')\n",
        "  \n",
        "  print(\"The size of the train data-------------------->\",X_train.shape)\n",
        "  print(\"The size of the train target data------------->\",y_train.shape)\n",
        "  print()\n",
        "  print(\"The size of the test data--------------------->\",X_test.shape)\n",
        "  print(\"The size of the test target data-------------->\",y_test.shape)\n",
        "  print()\n",
        "  print(\"The size of the validation data--------------->\",X_val.shape)\n",
        "  print(\"The size of the validation target data-------->\",y_val.shape)\n",
        "\n",
        "  print(\"Normalizing the data>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "  #normalize the data\n",
        "  X_train  = X_train/255.0\n",
        "  X_test = X_test/255.0\n",
        "  X_val = X_val/255.0\n",
        "\n",
        "  print(\"Converting target to one hot encoded values>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "  #convert to numerical values to 0,1\n",
        "  y_train = to_categorical(y_train,num_classes=3)\n",
        "  y_test = to_categorical(y_test,num_classes=3)\n",
        "  y_val = to_categorical(y_val,num_classes=3)\n",
        "\n",
        "  print(X_train[0].shape)\n",
        "  print(\"reshaping the data>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "  \n",
        "  X_train = X_train.reshape(X_train.shape[0] , 100 ,100 , 1)\n",
        "  X_test = X_test.reshape(X_test.shape[0] , 100 ,100 , 1)\n",
        "  X_val = X_val.reshape(X_val.shape[0] , 100 ,100 , 1)\n",
        "  \n",
        "  print(\"Preprocessing  completed!!!!!!!!!! stay happy :)\")\n",
        "  return X_train,y_train,X_test,y_test,X_val,y_val"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwdQmeLllMkA",
        "outputId": "e6de60fa-c082-4de4-aec8-e54044113265"
      },
      "source": [
        "X_train,y_train,X_test,y_test,X_val,y_val = preprocess_dataset(data_array, target_array)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffling the dataset...............\n",
            "spliting all data...................\n",
            "1199 149 149\n",
            "Converting to numpy array>>>>>>>>>>>>>>>>>>\n",
            "The size of the train data--------------------> (1200, 100, 100)\n",
            "The size of the train target data-------------> (1200,)\n",
            "\n",
            "The size of the test data---------------------> (149, 100, 100)\n",
            "The size of the test target data--------------> (149,)\n",
            "\n",
            "The size of the validation data---------------> (150, 100, 100)\n",
            "The size of the validation target data--------> (150,)\n",
            "Normalizing the data>>>>>>>>>>>>>>>>>>>>>>\n",
            "Converting target to one hot encoded values>>>>>>>>>>>>>>>>>>>>>>\n",
            "(100, 100)\n",
            "reshaping the data>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Preprocessing  completed!!!!!!!!!! stay happy :)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXuPkzcTlWjO",
        "outputId": "bb66c6be-7eeb-4d91-be10-8870f8ed049b"
      },
      "source": [
        "print(\"The size of the train data-------------------->\",X_train.shape)\n",
        "print(\"The size of the train target data------------->\",y_train.shape)\n",
        "print()\n",
        "print(\"The size of the test data--------------------->\",X_test.shape)\n",
        "print(\"The size of the test target data-------------->\",y_test.shape)\n",
        "print()\n",
        "print(\"The size of the validation data--------------->\",X_val.shape)\n",
        "print(\"The size of the validation target data-------->\",y_val.shape)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the train data--------------------> (1200, 100, 100, 1)\n",
            "The size of the train target data-------------> (1200, 3)\n",
            "\n",
            "The size of the test data---------------------> (149, 100, 100, 1)\n",
            "The size of the test target data--------------> (149, 3)\n",
            "\n",
            "The size of the validation data---------------> (150, 100, 100, 1)\n",
            "The size of the validation target data--------> (150, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2RDtIxDrO-r"
      },
      "source": [
        "# Build the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOIT-aY_m9bo"
      },
      "source": [
        "#building the model\n",
        "def build_model(input_shape , num_classes , ):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, (5, 5), activation='relu', padding='same', input_shape=input_shape)) \n",
        "  model.add(Conv2D(64, (5, 5), activation='relu', padding='same', ))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "  #model.add(Conv2D(64, (5, 5), activation='relu', padding='same', ))\n",
        "  #model.add(BatchNormalization())\n",
        "  #model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu',  padding='same', ))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', padding='same',))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), activation='relu', padding='same',))\n",
        "  model.add(Conv2D(256, (3, 3), activation='relu',  padding='same',))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(1024, activation='relu',))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1024, activation='relu', ))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(num_classes, activation='sigmoid'))\n",
        "\n",
        "\n",
        "  # compile model\n",
        "  \n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  #model summary\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-NQJeITrbPW",
        "outputId": "d57c9ae2-a9c9-4d62-99c5-cac898aa9892"
      },
      "source": [
        "model = build_model(input_shape = (100,100,1), num_classes = 3)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 100, 100, 64)      1664      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 100, 100, 64)      102464    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 100, 100, 64)      256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 50, 50, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 50, 50, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 50, 50, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 50, 50, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 25, 25, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 25, 25, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 25, 25, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 25, 25, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 36864)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              37749760  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 3075      \n",
            "=================================================================\n",
            "Total params: 40,015,043\n",
            "Trainable params: 40,014,147\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3Qxps_4riQK",
        "outputId": "c4487cd9-8ba4-4cc1-91b6-c601cb66f265"
      },
      "source": [
        "# set callbacks\n",
        "early_stoppping = EarlyStopping(monitor = 'val_loss',\n",
        "                                min_delta = 0.001,\n",
        "                                patience = 10,\n",
        "                                restore_best_weights=True)\n",
        "\n",
        "#set the global values\n",
        "epoches = 50\n",
        "batch_size = 64\n",
        "\n",
        "#fit the model\n",
        "history = model.fit(X_train, y_train,  \n",
        "          batch_size=batch_size,  \n",
        "          epochs=epoches,  \n",
        "          verbose=1,  \n",
        "          validation_data=(X_val, y_val),  \n",
        "          shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "19/19 [==============================] - 335s 18s/step - loss: 3.7862 - accuracy: 0.8442 - val_loss: 27.6131 - val_accuracy: 0.4133\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 333s 18s/step - loss: 0.1324 - accuracy: 0.9900 - val_loss: 31.5663 - val_accuracy: 0.2867\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 333s 18s/step - loss: 0.1786 - accuracy: 0.9925 - val_loss: 14.1652 - val_accuracy: 0.4133\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 333s 18s/step - loss: 0.0485 - accuracy: 0.9975 - val_loss: 24.3523 - val_accuracy: 0.3333\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 333s 18s/step - loss: 0.0118 - accuracy: 0.9975 - val_loss: 11.3262 - val_accuracy: 0.4867\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 331s 17s/step - loss: 0.0403 - accuracy: 0.9975 - val_loss: 64.0520 - val_accuracy: 0.3267\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 331s 17s/step - loss: 0.1629 - accuracy: 0.9967 - val_loss: 24.5464 - val_accuracy: 0.3533\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 330s 17s/step - loss: 0.0357 - accuracy: 0.9983 - val_loss: 21.6289 - val_accuracy: 0.3800\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 331s 17s/step - loss: 0.6443 - accuracy: 0.9742 - val_loss: 26.6336 - val_accuracy: 0.6600\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 331s 17s/step - loss: 0.1385 - accuracy: 0.9933 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 331s 17s/step - loss: 0.1165 - accuracy: 0.9967 - val_loss: 0.0324 - val_accuracy: 0.9867\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 330s 17s/step - loss: 0.0015 - accuracy: 0.9992 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 331s 17s/step - loss: 0.0012 - accuracy: 0.9992 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 330s 17s/step - loss: 9.6421e-07 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "12/19 [=================>............] - ETA: 1:59 - loss: 0.0070 - accuracy: 0.9974"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp3jCi7UrstO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}